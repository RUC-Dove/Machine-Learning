---
description: 来源：https://wizardforcel.gitbooks.io/dm-algo-top10/content/svm-1.html
---

# 支持向量机（SVM）-1.问题引入

## 一、定义

### 1、超平面

#### （1）定义

 **超平面（Hyperplane)** 是指n维空间中的n-1维的子空间，它将线性空间分割成为不相交的两个部分。\
例如：二维空间的超平面是一条直线；三维空间的超平面是一个平面

#### （2）超平面的数学表达式

n维空间中, 满足n元一次方程 $a_1x_1+a_2x_2+... a_nx_n=b$ 的所有点 $(x_1,x_2,...,x_n)$ 称为空间的一张 **超平面**（即广义平面）。\
假设 $w$ 和 $x$ 是n维向量: $w=(w_1,w_2,...,w_n)^T$ , $x=(x_1,x_2,...,x_n)^T$
则 $w^Tx=b$或者 $w^T x+b=0$ 表示一个超平面（Hyperplane）

#### （3）超平面数学表达式的几何意义（以 $w^Tx=b$ 为例）

首先由余弦夹角公式 $cos\theta =\frac {w^T x}{\|w\| \|x\|}$ 得到 $w^T x=cos\theta \|w\|\|x\|=b$ 这也就是： $$cos\theta\|x\|=\frac{b}{\|w\|}$$

<figure><img src="../.gitbook/assets/image (27).png" alt=""><figcaption></figcaption></figure>

如图所示，我们得到对于向量 $x$ ，其投影的长度 $OA=cos\theta\|x\|=\frac{b}{\|w\|}$ ，这意味着： $w^T x=b$ 的几何意义表示——所有在 $w$ 上投影长度为 $\frac{b}{|w|}$ 的向量集合，也就是如图所示的超平面，该平面与向量 $w$ 垂直且相交于点 $A$（即 $w$ 是该平面的法向量）

下面说明空间中任意一点 $x'$ 到超平面的距离 $d=\frac{w^T x'+b}{\|w\|}$（注：此时假定的是 $w^T x'+b=0$)\
如图所示，向量 $x'$ 到超平面的距离\
 $$d=BA=OA-OB=\frac{w^T x}{\|w\|}-\frac{w^T x'}{\|w\|}=-\frac{w^T x'+b}{\|w\|}$$\
（这里没有具体考虑正负号问题）

### 2、支持向量机

#### （1）定义

 **支持向量机（Support Vector Machine)** 是一种监督式学习的方法，可广泛地应用于统计分类以及回归分析。支持向量机属于一般化线性分类器，这族分类器的特点是他们能够同时最小化经验误差与最大化几何边缘区，因此支持向量机也被称为最大边缘区分类器。

支持向量机将向量映射到一个更高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面，分隔超平面使两个平行超平面的距离最大化。假定平行超平面间的距离或差距越大，分类器的总误差越小。

<figure><img src="../.gitbook/assets/image (21).png" alt=""><figcaption></figcaption></figure>

在上面的图中，a和b都可以作为分类超平面，但最优超平面只有一个，最优分类平面使间隔最大化。 那是不是某条直线比其他的更加合适呢? 我们可以凭直觉来定义一条评价直线好坏的标准:

距离样本太近的直线不是最优的，因为这样的直线对噪声敏感度高，泛化性较差。 因此我们的目标是找到一条直线（图中的最优超平面），离所有点的距离最远。 由此， SVM算法的实质是找出一个能够将某个值最大化的超平面，这个值就是超平面离所有训练样本的最小距离。这个最小距离用SVM术语来说叫做**间隔(margin)** 。

#### （2）计算最优超平面（线性分类）

我们通常希望分类的过程是一个机器学习的过程。这些数据点并不需要是![\mathbb{R}^2](http://upload.wikimedia.org/math/a/1/f/a1fd49f304c1094efe3fda098d5eaa5f.png)中的点，而可以是任意![\mathbb{R}^n](http://upload.wikimedia.org/math/3/0/c/30c28f76ef7517dbd19df4d4c683dbe6.png)的点，我们希望能够把这些点通过一个n-1维的超平面分开，通常这个被称为**线性分类器**。我们还希望找到分类最佳的平面，即使得属于两个不同类的数据点间隔最大的那个面，该面亦称为**最大间隔超平面**。如果我们能够找到这个面，那么这个分类器就称为**最大间隔分类器**。

<figure><img src="../.gitbook/assets/image (15).png" alt=""><figcaption></figcaption></figure>

如上图所示，假设中间的分割线为 $$wx+b=0$$&#x20;

考虑上面3个点 $A$ 、 $B$ 和 $C$。从图中我们可以确定 $A$ 是×类别的，然而 $C$ 我们是不太确定的， $B$还算能够确定。这样我们可以得出结论，我们更应该关心靠近中间分割线的点，让他们尽可能地远离中间线，而不是在所有点上达到最优。因为那样的话，要使得一部分点靠近中间线来换取另外一部分点更加远离中间线。同时这个所谓的超平面的的确把这两种不同形状的数据点分隔开来

我们令分类函数： $f(x)$  $s.t.$ 在超平面一边的数据点所对应的 $y$ 全是 -1 ，而在另一边全是 1

<figure><img src="../.gitbook/assets/image (6).png" alt=""><figcaption></figcaption></figure>

最优超平面可以有无数种表达方式，即通过任意的缩放 $w$和 $b$ 。 习惯上我们使用以下方式来表达最优超平面: $w^T x+b=1$ ，式中的 $x$ 表示离超平面距离较近的那些点，这也就是：支持向量的表达式为 $y(w^T x+b)=1$

由 $x$ 到超平面的距离公式以及 $w^T x+b=1$ 可以知道，距离 $$d=\frac{w^T x+b}{\|w\|}=\frac{1}{\|w\|}$$

由**间隔(Margin)**的定义可以知道， $M=2d=\frac{2}{\|w\|}$

要最大化 $M$，也就是要最小化 $\|w\|$，这也就是最小化 $\frac{1}{2}\|w\|^2$（这里添加系数和平方是为了方便求导）

综上，我们要解决的问题是： $min \frac{1}{2}\|w\|^2,s.t.y_i(w^T x_i+b)\geqslant1$\
这是一个拉格朗日优化问题，可以通过拉格朗日乘数法得到最优超平面的权重向量 $w$ 和偏置 $b$ 。

PS

1、咱们就要确定上述分类函数 $f(x)=w^T x+b$ 中的两个参数 $w$ 和 $b$

2、那如何确定他们呢？答案是寻找两条边界端或极端划分直线中间的最大间隔（之所以要寻最大间隔是为了能更好的划分不同类的点，为寻最大间隔，我们导出了导出了 $\frac{1}{2}\|w\|^2$ ，后面我们将引入拉格朗日函数和对偶变量 $a$，化为对单一因数对偶变量 $a$ 的求解），从而确定最终的最大间隔分类超平面和分类函数
